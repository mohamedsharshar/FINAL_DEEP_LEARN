# ğŸ« Ù…Ø´Ø±ÙˆØ¹ ÙƒØ´Ù Ø§Ù„Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙˆÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN)
## Pneumonia Detection using Convolutional Neural Networks

---

## ğŸ“š Ø§Ù„ÙÙ‡Ø±Ø³
1. [Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©](#Ù…Ù‚Ø¯Ù…Ø©-Ø¹Ù†-Ø§Ù„Ø´Ø¨ÙƒØ§Øª-Ø§Ù„Ø¹ØµØ¨ÙŠØ©-Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©)
2. [Ø´Ø±Ø­ CNN Ø¨Ø§Ù„ØªÙØµÙŠÙ„](#Ø´Ø±Ø­-cnn-Ø¨Ø§Ù„ØªÙØµÙŠÙ„)
3. [Ø´Ø±Ø­ GAN Ø¨Ø§Ù„ØªÙØµÙŠÙ„](#Ø´Ø±Ø­-gan-Ø¨Ø§Ù„ØªÙØµÙŠÙ„)
4. [Ø´Ø±Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹](#Ø´Ø±Ø­-Ø§Ù„Ù…Ø´Ø±ÙˆØ¹)
5. [Ø´Ø±Ø­ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯](#Ø´Ø±Ø­-Ø§Ù„Ø£ÙƒÙˆØ§Ø¯)

---

# ğŸ§  Ù…Ù‚Ø¯Ù…Ø© Ø¹Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©

## Ù…Ø§ Ù‡Ùˆ Deep LearningØŸ
Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.

```
Input â†’ [Hidden Layer 1] â†’ [Hidden Layer 2] â†’ ... â†’ [Hidden Layer N] â†’ Output
         (Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ©)        (Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ©)              (Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ©)
```

---

# ğŸ” Ø´Ø±Ø­ CNN Ø¨Ø§Ù„ØªÙØµÙŠÙ„

## Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„ØªÙ„Ø§ÙÙŠÙÙŠØ© (CNN)ØŸ

**CNN = Convolutional Neural Network**

Ù‡ÙŠ Ù†ÙˆØ¹ Ø®Ø§Øµ Ù…Ù† Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù…ØµÙ…Ù…Ø© Ø®ØµÙŠØµØ§Ù‹ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±. ØªØ­Ø§ÙƒÙŠ Ø·Ø±ÙŠÙ‚Ø© Ø¹Ù…Ù„ Ø§Ù„Ø¹ÙŠÙ† Ø§Ù„Ø¨Ø´Ø±ÙŠØ© ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·.

### ğŸ¯ Ù„Ù…Ø§Ø°Ø§ CNN Ù„Ù„ØµÙˆØ±ØŸ

| Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© | Ø§Ù„Ø­Ù„ Ù…Ø¹ CNN |
|---------------------------|-------------|
| Ø¹Ø¯Ø¯ Parameters Ø¶Ø®Ù… Ø¬Ø¯Ø§Ù‹ | Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Weight Sharing) |
| Ù„Ø§ ØªÙÙ‡Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© | Ø§Ù„Ù€ Convolution ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª |
| Ø­Ø³Ø§Ø³Ø© Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„ÙƒØ§Ø¦Ù† ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© | Ø§Ù„Ù€ Pooling ÙŠÙˆÙØ± Translation Invariance |

---

## ğŸ—ï¸ Ù…ÙƒÙˆÙ†Ø§Øª CNN Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

### 1ï¸âƒ£ Ø·Ø¨Ù‚Ø© Ø§Ù„ØªÙ„Ø§ÙÙŠÙ (Convolutional Layer)

```
Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©          Ø§Ù„ÙÙ„ØªØ± (Kernel)         Feature Map
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  2  3  4  â”‚         â”‚ 1  0  â”‚              â”‚ 8  12   â”‚
â”‚ 5  6  7  8  â”‚    *    â”‚ 0  1  â”‚      =       â”‚ 16 20   â”‚
â”‚ 9  10 11 12 â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 13 14 15 16 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ÙƒÙŠÙ ØªØ¹Ù…Ù„ØŸ**
- Ø§Ù„ÙÙ„ØªØ± (Kernel) ÙŠØªØ­Ø±Ùƒ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
- ÙŠØ¶Ø±Ø¨ Ø§Ù„Ù‚ÙŠÙ… ÙˆÙŠØ¬Ù…Ø¹Ù‡Ø§
- ÙŠÙ†ØªØ¬ Feature Map ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©

**Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÙŠ:**
```python
# ÙÙ„ØªØ± Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­ÙˆØ§Ù Ø§Ù„Ø£ÙÙ‚ÙŠØ©
horizontal_edge = [[-1, -1, -1],
                   [ 0,  0,  0],
                   [ 1,  1,  1]]

# ÙÙ„ØªØ± Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­ÙˆØ§Ù Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ©  
vertical_edge = [[-1, 0, 1],
                 [-1, 0, 1],
                 [-1, 0, 1]]
```

### 2ï¸âƒ£ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· ReLU

```
ReLU(x) = max(0, x)

Ù‚Ø¨Ù„ ReLU:  [-2, 5, -1, 3, -4, 7]
Ø¨Ø¹Ø¯ ReLU:  [ 0, 5,  0, 3,  0, 7]
```

**Ù„Ù…Ø§Ø°Ø§ ReLUØŸ**
- Ø¨Ø³ÙŠØ·Ø© ÙˆØ³Ø±ÙŠØ¹Ø© Ø§Ù„Ø­Ø³Ø§Ø¨
- ØªØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Vanishing Gradient
- ØªØ¶ÙŠÙ Non-linearity Ù„Ù„Ù†Ù…ÙˆØ°Ø¬

### 3ï¸âƒ£ Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ (Pooling Layer)

```
Max Pooling (2x2):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”
â”‚ 1  3 â”‚ 2  4 â”‚     â”‚ 3  4 â”‚
â”‚ 5  6 â”‚ 7  8 â”‚  â†’  â”‚ 6  8 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â””â”€â”€â”€â”€â”€â”˜
â”‚ 9  2 â”‚ 1  3 â”‚
â”‚ 4  6 â”‚ 5  7 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ø§Ù„ÙÙˆØ§Ø¦Ø¯:**
- ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Dimensionality Reduction)
- ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Parameters
- ØªÙˆÙÙŠØ± Translation Invariance

### 4ï¸âƒ£ Ø·Ø¨Ù‚Ø© Flatten

```
Feature Maps (3D)          Vector (1D)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ 1 2 3 â”‚ â”‚      â†’       â”‚ 1 2 3 4 5 6 ... â”‚
â”‚ â”‚ 4 5 6 â”‚ â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5ï¸âƒ£ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…ØªØµÙ„Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (Fully Connected)

```
Flattened Vector â†’ [FC Layer 1] â†’ [FC Layer 2] â†’ Output
                      (256)          (128)        (2)
                                               Normal/Pneumonia
```

---

## ğŸ“Š Ø¨Ù†ÙŠØ© CNN Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CNN Architecture                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Input     Conv    ReLU   Pool    Conv    ReLU   Pool    FC     â”‚
â”‚  Image  â†’  Layer â†’      â†’ Layer â†’ Layer â†’      â†’ Layer â†’ Layers â”‚
â”‚  (128x128) (32)          (64x64)  (64)          (32x32)  â†’ Outputâ”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù‡Ù…Ø© ÙÙŠ CNN

### Padding (Ø§Ù„Ø­Ø´Ùˆ)
```
Ø¨Ø¯ÙˆÙ† Padding:              Ù…Ø¹ Padding:
5x5 â†’ 3x3                  5x5 â†’ 5x5 (same size)
```

### Stride (Ø§Ù„Ø®Ø·ÙˆØ©)
```
Stride = 1: Ø§Ù„ÙÙ„ØªØ± ÙŠØªØ­Ø±Ùƒ Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø©
Stride = 2: Ø§Ù„ÙÙ„ØªØ± ÙŠØªØ­Ø±Ùƒ Ø®Ø·ÙˆØªÙŠÙ† (ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø­Ø¬Ù… Ø£Ø³Ø±Ø¹)
```

### Batch Normalization
- ÙŠÙØ·Ø¨Ù‘Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
- ÙŠØ³Ø±Ù‘Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- ÙŠÙ‚Ù„Ù„ Overfitting

### Dropout
```
Ù‚Ø¨Ù„ Dropout: [0.5, 0.3, 0.8, 0.2, 0.9]
Ø¨Ø¹Ø¯ Dropout: [0.5, 0.0, 0.8, 0.0, 0.9]  (Ø¨Ø¹Ø¶ Ø§Ù„Ù‚ÙŠÙ… = 0)
```
- ÙŠÙ…Ù†Ø¹ Overfitting
- ÙŠØ¬Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ© Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… features Ù…ØªØ¹Ø¯Ø¯Ø©

---

# ğŸ¨ Ø´Ø±Ø­ GAN Ø¨Ø§Ù„ØªÙØµÙŠÙ„

## Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ© Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ© (GAN)ØŸ

**GAN = Generative Adversarial Network**

Ù‡ÙŠ Ø¨Ù†ÙŠØ© ØªØªÙƒÙˆÙ† Ù…Ù† Ø´Ø¨ÙƒØªÙŠÙ† Ø¹ØµØ¨ÙŠØªÙŠÙ† ØªØªÙ†Ø§ÙØ³Ø§Ù† Ù…Ø¹ Ø¨Ø¹Ø¶Ù‡Ù…Ø§:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         GAN Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Random      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   Noise   â†’   â”‚ Generator â”‚   â†’    â”‚ Discriminator â”‚  â†’ Real/Fake â”‚
â”‚   (z)         â”‚  (Ø§Ù„Ù…ÙˆÙ„Ø¯)  â”‚        â”‚   (Ø§Ù„Ù…Ù…ÙŠØ²)    â”‚        â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                    â†‘                      â†‘                  â”‚
â”‚                    â”‚                      â”‚                  â”‚
â”‚               Fake Images            Real Images             â”‚
â”‚                                    (Ù…Ù† Dataset)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ Ù…ÙƒÙˆÙ†Ø§Øª GAN

### 1ï¸âƒ£ Ø§Ù„Ù…ÙˆÙ„Ø¯ (Generator)

**Ø§Ù„ÙˆØ¸ÙŠÙØ©:** ÙŠØ£Ø®Ø° Ø¶ÙˆØ¶Ø§Ø¡ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙˆÙŠÙ†ØªØ¬ ØµÙˆØ± Ù…Ø²ÙŠÙØ©

```python
class Generator(nn.Module):
    def __init__(self, latent_dim=100):
        super().__init__()
        self.model = nn.Sequential(
            # Input: Random noise (100,)
            nn.Linear(latent_dim, 256),
            nn.LeakyReLU(0.2),
            
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2),
            
            nn.Linear(1024, 784),  # 28x28 = 784
            nn.Tanh()  # Output: [-1, 1]
        )
    
    def forward(self, z):
        return self.model(z).view(-1, 1, 28, 28)
```

### 2ï¸âƒ£ Ø§Ù„Ù…Ù…ÙŠØ² (Discriminator)

**Ø§Ù„ÙˆØ¸ÙŠÙØ©:** ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„ØªÙ…ÙŠÙŠØ² Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙˆØ§Ù„Ù…Ø²ÙŠÙØ©

```python
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            # Input: Image (1, 28, 28)
            nn.Flatten(),
            
            nn.Linear(784, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            
            nn.Linear(256, 1),
            nn.Sigmoid()  # Output: probability [0, 1]
        )
    
    def forward(self, img):
        return self.model(img)
```

---

## âš”ï¸ ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŸ

### Ø§Ù„Ù„Ø¹Ø¨Ø© Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠØ© (Adversarial Game)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Loop                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Step 1: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ù…ÙŠØ² (Discriminator)                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  â€¢ Ø£Ø¹Ø·Ù‡ ØµÙˆØ± Ø­Ù‚ÙŠÙ‚ÙŠØ© â†’ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ‚ÙˆÙ„ "Real" (1)                â”‚
â”‚  â€¢ Ø£Ø¹Ø·Ù‡ ØµÙˆØ± Ù…Ø²ÙŠÙØ© â†’ ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ‚ÙˆÙ„ "Fake" (0)                 â”‚
â”‚                                                             â”‚
â”‚  Step 2: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆÙ„Ø¯ (Generator)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ ÙˆÙ„Ù‘Ø¯ ØµÙˆØ± Ù…Ø²ÙŠÙØ©                                           â”‚
â”‚  â€¢ Ø­Ø§ÙˆÙ„ Ø®Ø¯Ø§Ø¹ Ø§Ù„Ù…Ù…ÙŠØ² Ù„ÙŠÙ‚ÙˆÙ„ "Real"                           â”‚
â”‚  â€¢ Ø¥Ø°Ø§ Ø§Ù†Ø®Ø¯Ø¹ Ø§Ù„Ù…Ù…ÙŠØ² = Ø§Ù„Ù…ÙˆÙ„Ø¯ ÙŠØªØ­Ø³Ù†                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Loss Function)

```
Discriminator Loss:
L_D = -[log(D(x)) + log(1 - D(G(z)))]
      â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Real images   Fake images

Generator Loss:
L_G = -log(D(G(z)))
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      ÙŠØ±ÙŠØ¯ Ø®Ø¯Ø§Ø¹ D
```

---

## ğŸ”„ Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„ØªØ¯Ø±ÙŠØ¨ GAN

```python
# Training Loop
for epoch in range(num_epochs):
    for real_images, _ in dataloader:
        batch_size = real_images.size(0)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: Train Discriminator
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Real images
        real_labels = torch.ones(batch_size, 1)
        real_output = discriminator(real_images)
        d_loss_real = criterion(real_output, real_labels)
        
        # Fake images
        z = torch.randn(batch_size, latent_dim)
        fake_images = generator(z)
        fake_labels = torch.zeros(batch_size, 1)
        fake_output = discriminator(fake_images.detach())
        d_loss_fake = criterion(fake_output, fake_labels)
        
        # Total D loss
        d_loss = d_loss_real + d_loss_fake
        
        optimizer_D.zero_grad()
        d_loss.backward()
        optimizer_D.step()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: Train Generator
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        z = torch.randn(batch_size, latent_dim)
        fake_images = generator(z)
        output = discriminator(fake_images)
        
        # Generator wants D to think fake is real
        g_loss = criterion(output, real_labels)
        
        optimizer_G.zero_grad()
        g_loss.backward()
        optimizer_G.step()
```

---

## ğŸ“ˆ Ø£Ù†ÙˆØ§Ø¹ GAN Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©

| Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|-------|-------|----------|
| **DCGAN** | ÙŠØ³ØªØ®Ø¯Ù… Convolution | ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø© |
| **CGAN** | Conditional GAN | ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø¨Ø´Ø±ÙˆØ· Ù…Ø¹ÙŠÙ†Ø© |
| **CycleGAN** | ØªØ­ÙˆÙŠÙ„ Ø¨ÙŠÙ† domains | ØªØ­ÙˆÙŠÙ„ ØµÙˆØ± Ø§Ù„Ø®ÙŠÙˆÙ„ Ù„Ø­Ù…ÙŠØ± |
| **StyleGAN** | ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ù€ style | ØªÙˆÙ„ÙŠØ¯ ÙˆØ¬ÙˆÙ‡ ÙˆØ§Ù‚Ø¹ÙŠØ© |
| **Pix2Pix** | Image-to-Image | ØªØ­ÙˆÙŠÙ„ Ø±Ø³Ù… Ù„ØµÙˆØ±Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© |

---

## ğŸ†š Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† CNN Ùˆ GAN

| Ø§Ù„Ø®Ø§ØµÙŠØ© | CNN | GAN |
|---------|-----|-----|
| **Ø§Ù„Ù‡Ø¯Ù** | ØªØµÙ†ÙŠÙ/ØªØ¹Ø±Ù | ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© |
| **Ø§Ù„Ø¨Ù†ÙŠØ©** | Ø´Ø¨ÙƒØ© ÙˆØ§Ø­Ø¯Ø© | Ø´Ø¨ÙƒØªÙŠÙ† Ù…ØªÙ†Ø§ÙØ³ØªÙŠÙ† |
| **Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª** | ØµÙˆØ± | Ø¶ÙˆØ¶Ø§Ø¡ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© |
| **Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª** | ØªØµÙ†ÙŠÙ (class) | ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© |
| **Ø§Ù„ØªØ¯Ø±ÙŠØ¨** | Supervised | Unsupervised |
| **Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…** | ÙƒØ´Ù Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ØŒ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡ | ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±ØŒ ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ± |

---

# ğŸ¥ Ø´Ø±Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

## Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙŠØ³ØªØ®Ø¯Ù… **CNN** Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø±Ø¦ÙˆÙŠ Ù…Ù† ØµÙˆØ± Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ© Ù„Ù„ØµØ¯Ø±.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Project Pipeline                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  X-Ray    â†’   Preprocessing   â†’   CNN Model   â†’   Diagnosis â”‚
â”‚  Image        (ØªØ¬Ù‡ÙŠØ² Ø§Ù„ØµÙˆØ±Ø©)      (Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)       (Ø§Ù„ØªØ´Ø®ÙŠØµ)  â”‚
â”‚                                                              â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                                              â”‚   NORMAL    â”‚ â”‚
â”‚                                              â”‚     Ø£Ùˆ      â”‚ â”‚
â”‚                                              â”‚  PNEUMONIA  â”‚ â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©

**Ø§Ù„Ù…ØµØ¯Ø±:** [HuggingFace - keremberke/chest-xray-classification](https://huggingface.co/datasets/keremberke/chest-xray-classification)

| Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© | Ø§Ù„Ø¹Ø¯Ø¯ | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|----------|-------|----------|
| **Train** | 4,077 ØµÙˆØ±Ø© | ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ |
| **Validation** | 1,165 ØµÙˆØ±Ø© | Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ |
| **Test** | 582 ØµÙˆØ±Ø© | Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ |

**Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:**
- `0` = NORMAL (Ø³Ù„ÙŠÙ…) ğŸŸ¢
- `1` = PNEUMONIA (Ø§Ù„ØªÙ‡Ø§Ø¨ Ø±Ø¦ÙˆÙŠ) ğŸ”´

---

## ğŸ—ï¸ Ø¨Ù†ÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PneumoniaCNN Architecture                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Input: 128x128x1 (Grayscale X-Ray)                             â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Block 1: Conv(1â†’32) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Drop â”‚    â”‚
â”‚  â”‚          128x128x1  â†’  64x64x32                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Block 2: Conv(32â†’64) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropâ”‚    â”‚
â”‚  â”‚          64x64x32   â†’  32x32x64                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Block 3: Conv(64â†’128) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropâ”‚   â”‚
â”‚  â”‚          32x32x64   â†’  16x16x128                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Block 4: Conv(128â†’256) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropâ”‚  â”‚
â”‚  â”‚          16x16x128  â†’  8x8x256                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Flatten: 8x8x256 = 16,384                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ FC1: 16384 â†’ 256 â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)      â”‚    â”‚
â”‚  â”‚ FC2: 256 â†’ 128 â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)        â”‚    â”‚
â”‚  â”‚ FC3: 128 â†’ 1 â†’ Sigmoid                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  Output: Probability [0, 1]                                      â”‚
â”‚          > 0.5 = PNEUMONIA                                       â”‚
â”‚          â‰¤ 0.5 = NORMAL                                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ø¥Ø¬Ù…Ø§Ù„ÙŠ Parameters:** ~4.5 Ù…Ù„ÙŠÙˆÙ†

---

## âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨

| Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ | Ø§Ù„Ù‚ÙŠÙ…Ø© | Ø§Ù„Ø´Ø±Ø­ |
|---------|--------|-------|
| **IMG_SIZE** | 128 | Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØµØºÙŠØ± |
| **BATCH_SIZE** | 32 | Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± ÙÙŠ ÙƒÙ„ Ø¯ÙØ¹Ø© |
| **EPOCHS** | 25 | Ø¹Ø¯Ø¯ Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ |
| **Learning Rate** | 0.001 | Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… |
| **Optimizer** | Adam | Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„ØªØ­Ø³ÙŠÙ† |
| **Loss Function** | BCELoss | Binary Cross Entropy |

---

## ğŸ›¡ï¸ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ù†Ø¹ Overfitting

### 1. Data Augmentation (ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
```python
transforms.RandomRotation(15)      # Ø¯ÙˆØ±Ø§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠ Â±15Â°
transforms.RandomHorizontalFlip()  # Ù‚Ù„Ø¨ Ø£ÙÙ‚ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
transforms.RandomAffine(translate=(0.1, 0.1))  # Ø¥Ø²Ø§Ø­Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
```

### 2. Dropout
- `Dropout2d(0.25)` ÙÙŠ Ø·Ø¨Ù‚Ø§Øª Conv
- `Dropout(0.5)` ÙÙŠ Ø·Ø¨Ù‚Ø§Øª FC

### 3. Batch Normalization
- ÙŠÙØ·Ø¨Ù‘Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† Ø§Ù„Ø·Ø¨Ù‚Ø§Øª

### 4. Early Stopping
- ÙŠÙˆÙ‚Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªØ­Ø³Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ 5 epochs

### 5. Learning Rate Scheduler
```python
ReduceLROnPlateau(patience=3, factor=0.5)
# ÙŠÙ‚Ù„Ù„ LR Ø¨Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ù†ØµÙ Ø¥Ø°Ø§ Ù„Ù… ÙŠØªØ­Ø³Ù† Ù„Ù€ 3 epochs
```

### 6. Weighted Sampling
- ÙŠÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø©

---

## ğŸ“ˆ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…

### Confusion Matrix (Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ)

```
                    Predicted
                 NORMAL  PNEUMONIA
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    NORMAL    â”‚   TN    â”‚   FP    â”‚
Actual        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    PNEUMONIA â”‚   FN    â”‚   TP    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TN = True Negative  (Ø³Ù„ÙŠÙ… ÙˆØªÙˆÙ‚Ø¹ Ø³Ù„ÙŠÙ…) âœ…
TP = True Positive  (Ù…Ø±ÙŠØ¶ ÙˆØªÙˆÙ‚Ø¹ Ù…Ø±ÙŠØ¶) âœ…
FP = False Positive (Ø³Ù„ÙŠÙ… ÙˆØªÙˆÙ‚Ø¹ Ù…Ø±ÙŠØ¶) âš ï¸ False Alarm
FN = False Negative (Ù…Ø±ÙŠØ¶ ÙˆØªÙˆÙ‚Ø¹ Ø³Ù„ÙŠÙ…) âŒ Ø®Ø·ÙŠØ±!
```

### Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„ØµÙŠØºØ© | Ø§Ù„Ù…Ø¹Ù†Ù‰ |
|---------|--------|--------|
| **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© |
| **Sensitivity/Recall** | TP/(TP+FN) | Ù‚Ø¯Ø±Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø±Ø¶Ù‰ |
| **Specificity** | TN/(TN+FP) | Ù‚Ø¯Ø±Ø© Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£ØµØ­Ø§Ø¡ |
| **Precision** | TP/(TP+FP) | Ø¯Ù‚Ø© Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© |
| **AUC** | Area Under ROC Curve | Ø¬ÙˆØ¯Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒÙ„ÙŠØ© |

---

# ğŸ’» Ø´Ø±Ø­ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯

## 1ï¸âƒ£ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯

```python
# Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
import numpy as np                    # Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
import matplotlib.pyplot as plt       # Ø±Ø³Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
import torch                          # Ù…ÙƒØªØ¨Ø© PyTorch Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚
import torch.nn as nn                 # Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©
import torch.optim as optim           # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†
from torchvision import transforms    # ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ±
from datasets import load_dataset     # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† HuggingFace

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ø¨Ø°Ø±Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±ÙŠØ©
np.random.seed(42)
torch.manual_seed(42)

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ù‡Ø§Ø² (GPU Ø£Ùˆ CPU)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---

## 2ï¸âƒ£ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Configuration)

```python
class Config:
    IMG_SIZE = 128    # Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© (128x128 pixels)
    BATCH_SIZE = 32   # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ± ÙÙŠ ÙƒÙ„ Ø¯ÙØ¹Ø©
    EPOCHS = 25       # Ø¹Ø¯Ø¯ Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    LR = 0.001        # Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (Learning Rate)

config = Config()
```

**Ù„Ù…Ø§Ø°Ø§ Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ…ØŸ**
- `IMG_SIZE=128`: ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ³Ø±Ø¹Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- `BATCH_SIZE=32`: Ù…Ù†Ø§Ø³Ø¨ Ù„Ø°Ø§ÙƒØ±Ø© GPU/CPU
- `EPOCHS=25`: ÙƒØ§ÙÙŠ Ù„Ù„ØªØ¹Ù„Ù… Ù…Ø¹ Early Stopping
- `LR=0.001`: Ù‚ÙŠÙ…Ø© Ù‚ÙŠØ§Ø³ÙŠØ© Ù„Ù€ Adam optimizer

---

## 3ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª

```python
# ØªØ­Ù…ÙŠÙ„ Dataset Ù…Ù† HuggingFace
dataset = load_dataset("keremberke/chest-xray-classification", "full")

# Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª
train_data = dataset['train']       # 4077 ØµÙˆØ±Ø©
val_data = dataset['validation']    # 1165 ØµÙˆØ±Ø©
test_data = dataset['test']         # 582 ØµÙˆØ±Ø©

# Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙØ¦Ø§Øª
class_names = dataset['train'].features['labels'].names
# ['NORMAL', 'PNEUMONIA']
```

---

## 4ï¸âƒ£ Custom Dataset Class

```python
class ChestXrayDataset(Dataset):
    def __init__(self, hf_dataset, transform=None):
        self.dataset = hf_dataset      # Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† HuggingFace
        self.transform = transform      # Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    
    def __len__(self):
        return len(self.dataset)        # Ø¹Ø¯Ø¯ Ø§Ù„ØµÙˆØ±
    
    def __getitem__(self, idx):
        sample = self.dataset[idx]
        image = sample['image']         # Ø§Ù„ØµÙˆØ±Ø©
        label = sample['labels']        # Ø§Ù„ØªØµÙ†ÙŠÙ (0 Ø£Ùˆ 1)
        
        # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ RGB Ø¥Ø°Ø§ ÙƒØ§Ù†Øª grayscale
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª
        if self.transform:
            image = self.transform(image)
        
        return image, label
```

**Ù„Ù…Ø§Ø°Ø§ Ù†Ø­ØªØ§Ø¬ Custom DatasetØŸ**
- PyTorch DataLoader ÙŠØ­ØªØ§Ø¬ Dataset class
- ÙŠØ³Ù…Ø­ Ø¨ØªØ·Ø¨ÙŠÙ‚ transforms Ø¹Ù„Ù‰ ÙƒÙ„ ØµÙˆØ±Ø©
- ÙŠÙˆØ­Ø¯ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª

---

## 5ï¸âƒ£ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØµÙˆØ± (Transforms)

```python
# ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ù…Ø¹ Data Augmentation)
train_tf = transforms.Compose([
    transforms.Grayscale(1),                    # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ grayscale (Ù‚Ù†Ø§Ø© ÙˆØ§Ø­Ø¯Ø©)
    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),  # ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù…
    transforms.RandomRotation(15),              # Ø¯ÙˆØ±Ø§Ù† Ø¹Ø´ÙˆØ§Ø¦ÙŠ Â±15Â°
    transforms.RandomHorizontalFlip(),          # Ù‚Ù„Ø¨ Ø£ÙÙ‚ÙŠ Ø¹Ø´ÙˆØ§Ø¦ÙŠ
    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Ø¥Ø²Ø§Ø­Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
    transforms.ToTensor(),                      # ØªØ­ÙˆÙŠÙ„ Ù„Ù€ Tensor
    transforms.Normalize([0.5], [0.5])          # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ… [-1, 1]
])

# ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø¨Ø¯ÙˆÙ† Augmentation)
test_tf = transforms.Compose([
    transforms.Grayscale(1),
    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])
```

**Ø´Ø±Ø­ ÙƒÙ„ ØªØ­ÙˆÙŠÙ„:**

| Ø§Ù„ØªØ­ÙˆÙŠÙ„ | Ø§Ù„ÙˆØ¸ÙŠÙØ© |
|---------|---------|
| `Grayscale(1)` | Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„Ø³ÙŠÙ†ÙŠØ© Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯ |
| `Resize` | ØªÙˆØ­ÙŠØ¯ Ø­Ø¬Ù… ÙƒÙ„ Ø§Ù„ØµÙˆØ± |
| `RandomRotation` | ÙŠÙ…Ù†Ø¹ overfitting |
| `RandomHorizontalFlip` | ÙŠØ²ÙŠØ¯ ØªÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª |
| `ToTensor` | ÙŠØ­ÙˆÙ„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ø£Ø±Ù‚Ø§Ù… [0,1] |
| `Normalize` | ÙŠØ¬Ø¹Ù„ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† [-1,1] |

---

## 6ï¸âƒ£ Weighted Sampling

```python
# Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
train_labels = [sample['labels'] for sample in dataset['train']]
class_counts = np.bincount(train_labels)  # [1082, 2995]

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Ø¹ÙƒØ³ Ø§Ù„ØªÙƒØ±Ø§Ø±)
weights = 1. / class_counts  # Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ù‚Ù„ ØªØ­ØµÙ„ Ø¹Ù„Ù‰ ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰
sample_weights = weights[train_labels]

# Ø¥Ù†Ø´Ø§Ø¡ Sampler
sampler = WeightedRandomSampler(
    sample_weights, 
    len(sample_weights), 
    replacement=True
)
```

**Ù„Ù…Ø§Ø°Ø§ Weighted SamplingØŸ**
```
Ø¨Ø¯ÙˆÙ† Weighting:
NORMAL: 1082 (26.5%)
PNEUMONIA: 2995 (73.5%)
â†’ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø¯ ÙŠØªØ¹Ù„Ù… ØªÙˆÙ‚Ø¹ PNEUMONIA Ø¯Ø§Ø¦Ù…Ø§Ù‹!

Ù…Ø¹ Weighting:
ÙƒÙ„ ÙØ¦Ø© ØªØ¸Ù‡Ø± Ø¨Ù†ÙØ³ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
â†’ ØªØ¹Ù„Ù… Ù…ØªÙˆØ§Ø²Ù†
```

---

## 7ï¸âƒ£ Ø¨Ù†ÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (PneumoniaCNN)

```python
class PneumoniaCNN(nn.Module):
    def __init__(self, img_size=128):
        super().__init__()
        
        # Ø¯Ø§Ù„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ block ÙˆØ§Ø­Ø¯
        def block(inc, outc): 
            return nn.Sequential(
                nn.Conv2d(inc, outc, 3, padding=1),  # Convolution
                nn.BatchNorm2d(outc),                 # Batch Normalization
                nn.ReLU(True),                        # Activation
                nn.MaxPool2d(2),                      # Pooling (ÙŠÙ‚Ù„Ù„ Ø§Ù„Ø­Ø¬Ù… Ù„Ù„Ù†ØµÙ)
                nn.Dropout2d(0.25)                    # Dropout
            )
        
        # Feature Extractor (4 blocks)
        self.features = nn.Sequential(
            block(1, 32),    # 128â†’64, channels: 1â†’32
            block(32, 64),   # 64â†’32, channels: 32â†’64
            block(64, 128),  # 32â†’16, channels: 64â†’128
            block(128, 256)  # 16â†’8, channels: 128â†’256
        )
        
        # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù€ Flatten
        flat = 256 * (img_size // 16) ** 2  # 256 * 8 * 8 = 16384
        
        # Classifier (Fully Connected Layers)
        self.classifier = nn.Sequential(
            nn.Flatten(),                    # 16384
            nn.Linear(flat, 256),            # 16384 â†’ 256
            nn.BatchNorm1d(256),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(256, 128),             # 256 â†’ 128
            nn.BatchNorm1d(128),
            nn.ReLU(True),
            nn.Dropout(0.5),
            nn.Linear(128, 1),               # 128 â†’ 1
            nn.Sigmoid()                     # Output: [0, 1]
        )
    
    def forward(self, x):
        return self.classifier(self.features(x))
```

**ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:**
```
Input: (batch, 1, 128, 128)
    â†“ Block 1
(batch, 32, 64, 64)
    â†“ Block 2
(batch, 64, 32, 32)
    â†“ Block 3
(batch, 128, 16, 16)
    â†“ Block 4
(batch, 256, 8, 8)
    â†“ Flatten
(batch, 16384)
    â†“ FC Layers
(batch, 1)  â† Probability
```

---

## 8ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨

```python
# Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© (Binary Cross Entropy)
criterion = nn.BCELoss()

# Ø§Ù„Ù…Ø­Ø³Ù† (Adam)
optimizer = optim.Adam(model.parameters(), lr=config.LR)

# Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='min',      # Ù†Ø±ÙŠØ¯ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù€ loss
    factor=0.5,      # Ø§Ø¶Ø±Ø¨ LR ÙÙŠ 0.5
    patience=3       # Ø§Ù†ØªØ¸Ø± 3 epochs Ù‚Ø¨Ù„ Ø§Ù„ØªÙ‚Ù„ÙŠÙ„
)
```

### Early Stopping Class

```python
class EarlyStopping:
    def __init__(self, patience=5):
        self.patience = patience    # Ø¹Ø¯Ø¯ epochs Ù„Ù„Ø§Ù†ØªØ¸Ø§Ø±
        self.counter = 0            # Ø¹Ø¯Ø§Ø¯
        self.best_loss = None       # Ø£ÙØ¶Ù„ loss
        self.best_model = None      # Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    
    def __call__(self, loss, model):
        if self.best_loss is None or loss < self.best_loss:
            # ØªØ­Ø³Ù†! Ø§Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.best_loss = loss
            self.best_model = copy.deepcopy(model.state_dict())
            self.counter = 0
        else:
            # Ù„Ù… ÙŠØªØ­Ø³Ù†
            self.counter += 1
        
        # Ù‡Ù„ Ù†ØªÙˆÙ‚ÙØŸ
        return self.counter >= self.patience
```

---

## 9ï¸âƒ£ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨

```python
for epoch in range(config.EPOCHS):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    model.train()  # ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    train_loss, correct, total = 0, 0, 0
    
    for imgs, lbls in train_loader:
        # Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¬Ù‡Ø§Ø²
        imgs = imgs.to(device)
        lbls = lbls.float().unsqueeze(1).to(device)  # (batch,) â†’ (batch, 1)
        
        # Forward pass
        optimizer.zero_grad()       # Ù…Ø³Ø­ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        out = model(imgs)           # ØªÙˆÙ‚Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        loss = criterion(out, lbls) # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
        
        # Backward pass
        loss.backward()             # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
        optimizer.step()            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        train_loss += loss.item() * imgs.size(0)
        correct += ((out > 0.5) == lbls).sum().item()
        total += lbls.size(0)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ (Validation)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    model.eval()  # ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
    val_loss, val_correct, val_total = 0, 0, 0
    
    with torch.no_grad():  # Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø¨ ØªØ¯Ø±Ø¬Ø§Øª
        for imgs, lbls in val_loader:
            imgs = imgs.to(device)
            lbls = lbls.float().unsqueeze(1).to(device)
            
            out = model(imgs)
            val_loss += criterion(out, lbls).item() * imgs.size(0)
            val_correct += ((out > 0.5) == lbls).sum().item()
            val_total += lbls.size(0)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
    train_acc = correct / total
    val_acc = val_correct / val_total
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù€ scheduler
    scheduler.step(val_loss / val_total)
    
    # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
    if val_acc >= max(history['val_acc']):
        torch.save(model.state_dict(), 'best_model.pth')
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Early Stopping
    if early_stop(val_loss / val_total, model):
        print("Early stopping!")
        model.load_state_dict(early_stop.best_model)
        break
```

---

## ğŸ”Ÿ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

```python
# ØªØ­Ù…ÙŠÙ„ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

# Ø¬Ù…Ø¹ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
y_true, y_pred, y_prob = [], [], []

with torch.no_grad():
    for imgs, lbls in test_loader:
        out = model(imgs.to(device)).cpu()
        
        y_prob.extend(out.numpy())           # Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        y_pred.extend((out > 0.5).numpy())   # Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª (0 Ø£Ùˆ 1)
        y_true.extend(lbls.numpy())          # Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
accuracy = (y_pred == y_true).mean()

# ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
print(classification_report(y_true, y_pred, target_names=class_names))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')

# ROC Curve Ùˆ AUC
fpr, tpr, _ = roc_curve(y_true, y_prob)
roc_auc = auc(fpr, tpr)
```

---

## 1ï¸âƒ£1ï¸âƒ£ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```python
# Ø·Ø±ÙŠÙ‚Ø© 1: Ø­ÙØ¸ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙÙ‚Ø· (Ù…ÙØ¶Ù„Ø©)
torch.save(model.state_dict(), 'pneumonia_final.pth')

# Ø·Ø±ÙŠÙ‚Ø© 2: Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒØ§Ù…Ù„Ø§Ù‹
torch.save(model, 'pneumonia_complete.pth')

# Ø·Ø±ÙŠÙ‚Ø© 3: Ø­ÙØ¸ checkpoint ÙƒØ§Ù…Ù„ (Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±)
torch.save({
    'model_state': model.state_dict(),
    'model_name': 'PneumoniaCNN',
    'num_classes': 2,
    'img_size': 128,
    'class_names': ['NORMAL', 'PNEUMONIA']
}, 'pneumonia_checkpoint.pth')
```

**Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø·Ø±Ù‚:**

| Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© | Ø§Ù„Ø­Ø¬Ù… | Ø§Ù„Ù…Ø±ÙˆÙ†Ø© | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|---------|-------|---------|----------|
| `state_dict()` | ØµØºÙŠØ± | ØªØ­ØªØ§Ø¬ ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ | Production |
| `save(model)` | ÙƒØ¨ÙŠØ± | Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… | Quick testing |
| `checkpoint` | Ù…ØªÙˆØ³Ø· | ÙŠØ­ØªÙˆÙŠ metadata | Training continuation |

---

## ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø©

| Ø§Ù„Ù…Ù„Ù | Ø§Ù„ÙˆØµÙ |
|-------|-------|
| `best_model.pth` | Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ |
| `pneumonia_final.pth` | Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© |
| `pneumonia_complete.pth` | Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„ |
| `pneumonia_checkpoint.pth` | Checkpoint Ù…Ø¹ metadata |
| `training_history.png` | Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ |
| `confusion_matrix.png` | Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ |
| `roc_curve.png` | Ù…Ù†Ø­Ù†Ù‰ ROC |
| `predictions.png` | Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª |

---

## ğŸš€ ÙƒÙŠÙÙŠØ© ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

```bash
# 1. ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
pip install datasets torch torchvision tqdm scikit-learn seaborn matplotlib pillow

# 2. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù€ Notebook
jupyter notebook pneumonia_detection_notebook_documented_CNN.ipynb
```

---

## ğŸ“š Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹

- [PyTorch Documentation](https://pytorch.org/docs/)
- [HuggingFace Datasets](https://huggingface.co/docs/datasets/)
- [CNN Explained](https://cs231n.github.io/convolutional-networks/)
- [GAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

---

**ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø±Ø­ Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ØªØ®Ø±Ø¬ ÙÙŠ Ù…Ø§Ø¯Ø© Neural Network and Deep Learning** ğŸ“
